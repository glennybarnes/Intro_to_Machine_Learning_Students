{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP\n",
    "\n",
    "## Natural Language Processing - NLP\n",
    "\n",
    "NLP is processing natual language - free text and speech. We can use free text in predictive modelling, in fact it is a quickly developing field.\n",
    "\n",
    "Technologies such as speech recognition, automatic translation, and computer speech are all based on the concpets that we'll cover here. \n",
    "\n",
    "The premise of NLP is that we take a piece of text and process it to transform into a format that we can process. In our case here we'll take free text and convert it into a set of features - we can then use those features to make predictions for our target, just like always!\n",
    "\n",
    "#### Example Exercise - Spam Filtering\n",
    "\n",
    "For an example we'll build a spam filter. The dataset here has two columns - one is a text message, the other is a human assigned label of spam or ham. We want to be able to detect the spam messages and filter them out. The only feature we have to be able to do so is the message itself..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding=\"ISO-8859-1\")\n",
    "df.drop(columns={\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"}, inplace=True)\n",
    "df.rename(columns={\"v1\":\"target\", \"v2\":\"text\"}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Construction\n",
    "\n",
    "Unlike the data that we are used to, our free text doesn't really have a set of features, only one feature that contains random length snipits of text. Feeding in random text messages to a predictive algorithm is unlikely to be effective. The first step in using natural text as an input for our predictive models is to transform our data into a usable feature set that we can feed into a model. This transformation will result in our 1 dimension (kind of) free text turning into a (probably) very high dimension set of features. \n",
    "\n",
    "### Tokenization\n",
    "\n",
    "The first step in transforming the data is to extract each word from the original text - this process is called Tokenizing. Tokenizing takes a sentance and transforms it into a list of tokens - (roughly) the words in the sentence. \n",
    "\n",
    "![Tokenization](images/tokenization.png \"Tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Very Simple Tokenizer\n",
    "\n",
    " We can visualize the process of tokenization pretty easily by looking at an example of a dead simple tokenizer. The function below will tokenize a sentence in a basic way - it will chop apart the sentence into words, and add them to a list. This example uses regex to do basic filtering to only extract words that are 2+ letters. \n",
    "\n",
    "<b>Note:</b> This example of a tokenizer (and this stuff in general) is a very basic version, and the field of NLP is developing quickly. More advanced text processing is better able to capture more of the structure of the language, and more of the meaning. We are stripping lots of \"hidden\" meaning out to make it manageable, more advanced NLP tries to understand as much of that meaning as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Really simple tokenizer\n",
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    for token in re.findall(r\"\\b\\w\\w+\\b\", sentence):\n",
    "        tokens.append(token.lower())\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize a Thing...\n",
    "\n",
    "We can look at the example of one of our sentences being transformed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\n",
      "['freemsg', 'hey', 'there', 'darling', 'it', 'been', 'week', 'now', 'and', 'no', 'word', 'back', 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'to', 'send', '50', 'to', 'rcv']\n"
     ]
    }
   ],
   "source": [
    "tolk = tokenize(df[\"text\"][5])\n",
    "print(df[\"text\"][5])\n",
    "print(tolk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Results\n",
    "\n",
    "Tokenizing transforms our random text into something more orderly and able to be processed - in this case a list of words. This tokenization process is the basis of all other processing. \n",
    "\n",
    "We can take this set of tokens and do some further processing. For this we'll use something called a Vectorizer. The vectorizer will do the simple act of tokenizing, and build the actual data structure that we need as a feature set. \n",
    "\n",
    "#### Vocabulary\n",
    "\n",
    "The set of all our tokens, or all words used in our dataset is called the vocabulary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizers\n",
    "\n",
    "In sklearn we have some libraries called vectorizors - they can do much of the text processing for us. There are two that we'll touch on - CountVectorizer and Tf-idf Vectorizer. \n",
    "\n",
    "Each of these does the bulk of the prep for us:\n",
    "<ul>\n",
    "<li> Tokenize the strings. \n",
    "<li> Count the occurances of each. \n",
    "<li> Weight the relative importance of different words. In different ways...\n",
    "<li> Produce a usable feature set. \n",
    "</ul> \n",
    "\n",
    "<b> Each takes in a dataset of text strings and outputs a set of features that we can use for our predictions. </b>\n",
    "\n",
    "### Embedding\n",
    "\n",
    "The outcome of any of these vectorizers is a numerical representation of the text that we are using. This is critical to our model as we need something that we can \"math\" for our model to work. There are many ways to generate these values, and generating them well is a critical step in making language models. We will look at several, for now focus on the key idea - we what to turn text into a set of numbers that will become our features. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization\n",
    "\n",
    "Count vectorization is the most simple process we can use to make our text into a set of features. The count vectorization will split apart our data into tokens, count them up, and produce an array where:\n",
    "<ul>\n",
    "<li> Each column is a word. \n",
    "<li> Each row is an input piece of text (e.g. an email)\n",
    "<li> Each cell is a count of the number of times that word appears. \n",
    "</ul>\n",
    "\n",
    "![Count Vectorization](images/count_vector.png \"Count Vectorization\")\n",
    "\n",
    "This is our Bag of Words - now instead of having a sentence as an input, we have something like a one-hot matrix of words used. We can picture this by printing it out (Note: there's a little reconstruction below to put it into a nice dataframe format)\n",
    "\n",
    "#### Count Vectorizer Benefits and Drawbacks\n",
    "\n",
    "The main benefit of the count vectorizer is the simplicity and speed - all it needs to do is count. It has the downside of being quite simple in the analysis of the language - we don't extract which words are more or less important, we just get a count. For things that are written similarly this can be effective - I have used this for a simple tool to detect cheaters on tests - people copying from each other or a common source like Chegg tend to have the same words repeated in their answer. \n",
    "\n",
    "#### Sparse Features\n",
    "\n",
    "This process generally produces a sparse matrix - most words are not in most sentences, so most scores in the final matrix are 0. For this we'll keep it simple and use algorithms that deal with sparse matrices (e.g. SVC). Later on we'll look at ways to reduce dimensionality.\n",
    "\n",
    "Some algorithms may throw an error if you feed them a sparse matrix. \n",
    "\n",
    "### Use Count Vectorizer\n",
    "\n",
    "We can look at the dataset that is generated for us after using the count vectorizer. The mechanics are very similar to the other sklearn transformers that we've used. The output is an array, so there's a little extra code there to put it into a dataframe for easy viewing. For the first try I'll set a limit of 150 features, so only the most common 150 tokens will be kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 150)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>after</th>\n",
       "      <th>all</th>\n",
       "      <th>already</th>\n",
       "      <th>am</th>\n",
       "      <th>amp</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>won</th>\n",
       "      <th>work</th>\n",
       "      <th>www</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>ì_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4777</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3523</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4930</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      about  after  all  already  am  amp  an  and  any  are  ...  will  with  \\\n",
       "1154      0      0    1        0   0    0   0    1    0    0  ...     0     0   \n",
       "4777      0      0    0        0   0    0   0    1    0    0  ...     0     0   \n",
       "3523      0      0    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "2025      0      0    0        0   0    0   0    1    0    1  ...     0     0   \n",
       "426       0      0    0        0   0    0   0    0    0    1  ...     0     0   \n",
       "400       0      1    0        0   0    0   0    0    0    1  ...     0     0   \n",
       "1681      0      0    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "5477      0      0    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "4930      0      0    0        0   0    0   0    0    0    0  ...     0     0   \n",
       "1679      0      0    1        0   0    0   0    0    0    0  ...     1     0   \n",
       "\n",
       "      won  work  www  yeah  yes  you  your  ì_  \n",
       "1154    0     0    0     0    0    0     0   0  \n",
       "4777    0     0    0     0    0    0     0   0  \n",
       "3523    0     0    0     1    0    0     0   0  \n",
       "2025    0     0    0     0    1    1     0   0  \n",
       "426     0     0    0     0    0    1     0   0  \n",
       "400     0     0    0     0    0    0     0   0  \n",
       "1681    0     0    0     0    0    0     0   0  \n",
       "5477    0     1    0     0    0    0     0   0  \n",
       "4930    0     0    0     0    0    4     0   0  \n",
       "1679    0     0    0     0    0    0     0   0  \n",
       "\n",
       "[10 rows x 150 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_cv = CountVectorizer(max_features=150)\n",
    "tmp = vec_cv.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_cv.get_feature_names_out()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "tok_df.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That has the number of features limited to 150, if we imposed no limits we'd get something way messier... If we look at some of the words that we can see in the columns below, we can surmise that we are probably getting a bunch of junk that isn't all that useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 8672)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>ó_</th>\n",
       "      <th>û_</th>\n",
       "      <th>û_thanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5308</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 8672 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  000pes  008704050406  0089  0121  01223585236  01223585334  \\\n",
       "3980   0    0       0             0     0     0            0            0   \n",
       "1996   0    0       0             0     0     0            0            0   \n",
       "3264   0    0       0             0     0     0            0            0   \n",
       "1455   0    0       0             0     0     0            0            0   \n",
       "4152   0    0       0             0     0     0            0            0   \n",
       "663    0    0       0             0     0     0            0            0   \n",
       "2157   0    0       0             0     0     0            0            0   \n",
       "3884   0    0       0             0     0     0            0            0   \n",
       "5308   0    0       0             0     0     0            0            0   \n",
       "1514   0    0       0             0     0     0            0            0   \n",
       "\n",
       "      0125698789  02  ...  ó_  û_  û_thanks  ûªm  ûªt  ûªve  ûï  ûïharry  ûò  \\\n",
       "3980           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "1996           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "3264           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "1455           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "4152           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "663            0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "2157           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "3884           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "5308           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "1514           0   0  ...   0   0         0    0    0     0   0        0   0   \n",
       "\n",
       "      ûówell  \n",
       "3980       0  \n",
       "1996       0  \n",
       "3264       0  \n",
       "1455       0  \n",
       "4152       0  \n",
       "663        0  \n",
       "2157       0  \n",
       "3884       0  \n",
       "5308       0  \n",
       "1514       0  \n",
       "\n",
       "[10 rows x 8672 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_cv2 = CountVectorizer()\n",
    "tmp2 = vec_cv2.fit_transform(df[\"text\"])\n",
    "tok_cols2 = vec_cv2.get_feature_names_out()\n",
    "tok_df2 = pd.DataFrame(tmp2.toarray(), columns=tok_cols2)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp2.shape)\n",
    "tok_df2.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Vectorizer Results\n",
    "\n",
    "Each of the vectorizations above delivered us a fully formed feature set. Each row is one piece of our input text, each column is a word, and each cell is the count of the number of times that word occurs in that text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "\n",
    "TF-IDF vectorization is similar to the count vectorizor, but it does some calculations to determine the importance of the word. The calculations are based on the name:\n",
    "<ul>\n",
    "<li> <b>Term Frequency</b> - the number of times a word appears in a document divided by the total number of words in the document.\n",
    "<li> <b>Inverse Document Frequency</b> - the log of the total number of documents divided by the number of documents that contain the word.\n",
    "<li> <b>Note:</b> each input phrase (row in dataset) is a document. \n",
    "</ul>\n",
    "\n",
    "![TF-IDF](images/tfidf.png \"TF-IDF\" )\n",
    "\n",
    "The final result is the two multiplied by each other, hence TF-IDF. \n",
    "\n",
    "#### TF-IDF Importance\n",
    "\n",
    "TF-IDF weights the importance of each word to give lower scores to words that are:\n",
    "<ul>\n",
    "<li> <b>Too frequent</b> - words that repeat constantly are likely to not be helpful in differentiating sentences. \n",
    "    <ul>\n",
    "    <li> \"the\", \"it\", \"and\", \"to\", \"for\", etc. and other common words occur in a huge proportion of documents, so they are not very useful in differentiating between documents.\n",
    "    <li> In specific applications, other words that are common in that domain may also become too frequent.\n",
    "    </ul>\n",
    "<li> <b>Too rare</b> - words that almost never occur don't exist often enough to establish a pattern. \n",
    "    <ul>\n",
    "    <li> If words only extremely occasionally in our dataset, those rare words are not likely to be useful in differentiating between documents, since we just don't see them enough to establish any sort of pattern.\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "TF-IDF tends to give us a better ability to evaluate work importance, but it is still not able to extract relationships between words nor generate more sophisticated meaning of the words. For that we need to use more sophisticated processing libraries, such as word2vec that we'll look at later on. \n",
    "\n",
    "<b>Note:</b> a small change here in the max features argument, now it'll keep the 150 overall highest scoring tokens. This is slightly differnet from the most frequent, as those words that score poorly in the td-idf calculation will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 150)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>after</th>\n",
       "      <th>all</th>\n",
       "      <th>already</th>\n",
       "      <th>am</th>\n",
       "      <th>amp</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>won</th>\n",
       "      <th>work</th>\n",
       "      <th>www</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>ì_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193214</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.543048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.598621</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.565760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      about  after  all  already        am  amp   an       and  any  are  ...  \\\n",
       "672     0.0    0.0  0.0      0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  ...   \n",
       "954     0.0    0.0  0.0      0.0  0.000000  0.0  0.0  0.543048  0.0  0.0  ...   \n",
       "3946    0.0    0.0  0.0      0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  ...   \n",
       "199     0.0    0.0  0.0      0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  ...   \n",
       "4102    0.0    0.0  0.0      0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  ...   \n",
       "5216    0.0    0.0  0.0      0.0  0.481309  0.0  0.0  0.000000  0.0  0.0  ...   \n",
       "3056    0.0    0.0  0.0      0.0  0.400498  0.0  0.0  0.000000  0.0  0.0  ...   \n",
       "2254    0.0    0.0  0.0      0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  ...   \n",
       "3094    0.0    0.0  0.0      0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  ...   \n",
       "5518    0.0    0.0  0.0      0.0  0.000000  0.0  0.0  0.000000  0.0  0.0  ...   \n",
       "\n",
       "          will      with  won  work  www  yeah  yes       you      your   ì_  \n",
       "672   0.000000  0.224066  0.0   0.0  0.0   0.0  0.0  0.000000  0.193214  0.0  \n",
       "954   0.000000  0.000000  0.0   0.0  0.0   0.0  0.0  0.000000  0.598621  0.0  \n",
       "3946  0.000000  0.000000  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.0  \n",
       "199   0.000000  0.000000  0.0   0.0  0.0   0.0  0.0  0.241563  0.000000  0.0  \n",
       "4102  0.000000  0.192556  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.0  \n",
       "5216  0.425866  0.000000  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.0  \n",
       "3056  0.000000  0.000000  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.0  \n",
       "2254  0.000000  0.000000  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.0  \n",
       "3094  0.000000  0.565760  0.0   0.0  0.0   0.0  0.0  0.000000  0.000000  0.0  \n",
       "5518  0.000000  0.000000  0.0   0.0  0.0   0.0  0.0  0.128924  0.000000  0.0  \n",
       "\n",
       "[10 rows x 150 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF\n",
    "vec_tf = TfidfVectorizer(max_features=150)\n",
    "tmp = vec_tf.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_tf.get_feature_names_out()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "tok_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization Parameters\n",
    "\n",
    "There are several parameters that can be pretty important when doing vectorization:\n",
    "<ul>\n",
    "<li> Max Features - as seen above. Limits how many feature columns are produced. This will cap it to the N most frequent words instead of every word seen. \n",
    "<li> strip_accents - remove random characters such as accents. \n",
    "<li> lowercase - covert all to lower case. This is helpful as case matters in code, but doesn't matter for us. \n",
    "<li> stop_words - filter out stop words. More on this later. \n",
    "<li> tokenizer - we can specify our own tokenizer function, where we can layer in more processing. More on this later. \n",
    "<li> ngram_range - how \"big\" can tokens be? I.e. can you have a 2 word token - e.g. \"downhill skiing\". \n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Grams\n",
    "\n",
    "N-grams are a way to capture the relationship between words. For example, the phrase \"downhill skiing\" is a 2-gram, those two words together are a specific \"thing\", different from the words \"downhill\" and \"skiing\" by themselves. The N in n-grams is the number of words in the phrase that are stored as one token. Allowing n-grams that are longer than one word cam be extremely helpful in allowing our feature set to better capture the meaning of our text, it is common to have multi-word terms-of-art, product names, descriptions (e.g. \"dirty blonde\"), and so on. On the (potential) downside, it can increase the number of features dramatically, and can make the feature set more sparse. When allowing larger n-grams, particularly, it is common to limit the number of features up front and/or use some techniques later on to reduce the dimensionality of the feature set. A feature set that is 20 times wider than it is tall is probably not going to be ideal. \n",
    "\n",
    "![N-Gram Vectorization](images/ngram.png \"N-Gram Vectorization\" )\n",
    "\n",
    "##### N-Gram Vectorization\n",
    "\n",
    "We can allow longer n-grams with a hyperparameter, for this example I allowed things up to 3, we can see that the number of features will explode in size as now any up to n-word long sequence can be a feature in our feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 150)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 in</th>\n",
       "      <th>00 in our</th>\n",
       "      <th>00 per</th>\n",
       "      <th>00 sub</th>\n",
       "      <th>00 sub 16</th>\n",
       "      <th>00 subs</th>\n",
       "      <th>00 subs 16</th>\n",
       "      <th>000</th>\n",
       "      <th>000 bonus</th>\n",
       "      <th>...</th>\n",
       "      <th>ûò is limping</th>\n",
       "      <th>ûò sound</th>\n",
       "      <th>ûò sound ok</th>\n",
       "      <th>ûò to</th>\n",
       "      <th>ûò to an</th>\n",
       "      <th>ûò very</th>\n",
       "      <th>ûò very entertaining</th>\n",
       "      <th>ûówell</th>\n",
       "      <th>ûówell done</th>\n",
       "      <th>ûówell done û_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5431</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 104564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  00 in  00 in our  00 per  00 sub  00 sub 16  00 subs  00 subs 16  \\\n",
       "4351  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "1250  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "658   0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "1338  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "4958  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "5431  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "2757  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "4354  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "3227  0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "915   0.0    0.0        0.0     0.0     0.0        0.0      0.0         0.0   \n",
       "\n",
       "           000  000 bonus  ...  ûò is limping  ûò sound  ûò sound ok  ûò to  \\\n",
       "4351  0.000000        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "1250  0.000000        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "658   0.000000        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "1338  0.000000        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "4958  0.000000        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "5431  0.000000        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "2757  0.000000        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "4354  0.000000        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "3227  0.092395        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "915   0.000000        0.0  ...            0.0       0.0          0.0    0.0   \n",
       "\n",
       "      ûò to an  ûò very  ûò very entertaining  ûówell  ûówell done  \\\n",
       "4351       0.0      0.0                   0.0     0.0          0.0   \n",
       "1250       0.0      0.0                   0.0     0.0          0.0   \n",
       "658        0.0      0.0                   0.0     0.0          0.0   \n",
       "1338       0.0      0.0                   0.0     0.0          0.0   \n",
       "4958       0.0      0.0                   0.0     0.0          0.0   \n",
       "5431       0.0      0.0                   0.0     0.0          0.0   \n",
       "2757       0.0      0.0                   0.0     0.0          0.0   \n",
       "4354       0.0      0.0                   0.0     0.0          0.0   \n",
       "3227       0.0      0.0                   0.0     0.0          0.0   \n",
       "915        0.0      0.0                   0.0     0.0          0.0   \n",
       "\n",
       "      ûówell done û_  \n",
       "4351             0.0  \n",
       "1250             0.0  \n",
       "658              0.0  \n",
       "1338             0.0  \n",
       "4958             0.0  \n",
       "5431             0.0  \n",
       "2757             0.0  \n",
       "4354             0.0  \n",
       "3227             0.0  \n",
       "915              0.0  \n",
       "\n",
       "[10 rows x 104564 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDF\n",
    "vec_tf_ng = TfidfVectorizer(ngram_range=(1,3))\n",
    "tmp_ng = vec_tf_ng.fit_transform(df[\"text\"])\n",
    "tok_cols_ng = vec_tf_ng.get_feature_names_out()\n",
    "tok_df_ng = pd.DataFrame(tmp_ng.toarray(), columns=tok_cols_ng)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "tok_df_ng.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "What we have generated here, either with the count vectorizer or the tf-idf vectorizer, is a set of features that represent the meaning of the original text - or our \"embeddings\". The original words are not useful if we want to use some predictive models, we need to convert them into numbers. The features that we will ultimately feed to the model are the values in the cells of the matrix. We are left with:\n",
    "<ul>\n",
    "<li> Each row is still one instance, or one piece of text from our original dataset. </li>\n",
    "<li> Each column is a feature, or some thing that we know about each instance of our data. </li>\n",
    "<li> Each cell is a value, or the value of that feature for that instance. </li>\n",
    "</ul>\n",
    "\n",
    "This is similar to generating a pixel-based set of values for an image. We transform the original data into a new set of data that is good for our model. Here, those features are some variation of 'count' of the words in the original text. If we created any other type of features, this idea is still the same, but the 'meaning' part of the features would be different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Model with Text\n",
    "\n",
    "We now have enough tools to use our text as a feature set, and train a predictive model. We can probably be a bit smarter with how we process our text, but for a first pass, we have something that works. Try running the code below with each vectorizer. \n",
    "\n",
    "Support vector machines are a good choice for text classification, as they are able to handle sparse feature sets without adaptation, and the model tends to deliver accurate predictions for this type of problem. The sparse feature set thing is a small concern for now, we'll look at ways to reduce the dimensionality of the feature set later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/l4n1h3z533nbg7ks96r5chn80000gn/T/ipykernel_1262/553917011.py:18: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  pipe1.fit(X_train, y_train.ravel())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1210\n",
      "        spam       0.97      0.85      0.91       183\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.97      0.92      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svc = SVC()\n",
    "\n",
    "vec_cv = CountVectorizer(max_features=150, ngram_range=(1,2))\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "                    (\"vect\", vec_cv),\n",
    "                    (\"model\", model_svc)\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe1.fit(X_train, y_train.ravel())\n",
    "preds = pipe1.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))\n",
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Our predictions are pretty good!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Try with TD-IDF\n",
    "\n",
    "Try the previous prediction with the td-idf vectorizer. Play around with the ngrams if you have time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/l4n1h3z533nbg7ks96r5chn80000gn/T/ipykernel_1262/2199786220.py:18: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  pipe1.fit(X_train, y_train.ravel())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98      1204\n",
      "        spam       0.97      0.83      0.89       189\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.97      0.91      0.94      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svc = SVC()\n",
    "\n",
    "vec_tf = TfidfVectorizer(max_features=150)\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", model_svc)\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe1.fit(X_train, y_train.ravel())\n",
    "preds = pipe1.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))\n",
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Elaborate Language Processing\n",
    "\n",
    "In the example above we've done a \"base\" level of modelling - we transform the free text into something that we can process (the bag of words), and we can make predictions from it much like we would with any other one-hot encoded data. This process works fine, and it does deliver some pretty accurate results on our test data. \n",
    "\n",
    "To create NLP models that are more functional we can add some layers to our processing of the text to improve our understanding of the nuances of our text. Some things we can do are:\n",
    "<ul>\n",
    "<li> <b>Remove Stop Words</b> - common words like \"it\", \"a\", \"the\" are normally not all that useful in predicting the meaning, we can filter these out. \n",
    "<li> <b>Stemming</b> - coverting words down to their \"stem\". E.g. \"reasoning\" to \"reason\"\n",
    "<li> <b>Lemmatization</b> - similar to stemming, but tries to identify the correct stem contextually. E.g. \"Operating systems\" probably shouldn't become \"operate\" and \"system\"\n",
    "</ul>\n",
    "\n",
    "In general, stemming increases recall while harming precision. Lemmatization has similar impacts, but tends to be less aggressive, so the effects are smaller. The specific results are highly variable depending on the exact text that is used. Something that uses specific variations of words to mean specific things (e.g. science) is more likely to get no benefit or be negatively impacted - e.g. \"conditonally\" used in the context of a \"conditionally approved loan\" is probably not well represented by changing it to \"condition\". \n",
    "\n",
    "### NLTK Library\n",
    "\n",
    "NLTK is a library that provides a bunch of language processing stuff that we can use such as stop words and tokenizers. We'll leverage it here to make custom tokenizers to incorporate some of those features above. The things we are downloading here are pre-made sets of data, like stop words, and pretrained lists of \"root words\" that we can use to break words down into their root format. \n",
    "\n",
    "<b>Note:</b> the \"for package\" part there downloads the wordsets to your computer. NLTK has these prebuilt libraries of data that allow for the functions to do the stop words, stemming, and lemmatization. It might take a minute the first time you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/glennbarnes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/glennbarnes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/glennbarnes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "for package in ['stopwords','punkt','wordnet']:\n",
    "    nltk.download(package)\n",
    "    \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Tokenizers\n",
    "\n",
    "The vecorization libraries in sklearn allow you do specify the function to use to do tokenization. We can use this to include other processing that we'd like as part of the process, such as removing stop words or stemming. The tokenizer functions below can, potentially, contain anything you'd like. As long as the call function returns a list of tokens, it should work. \n",
    "\n",
    "<b>Note:</b> if you look up examples, these functions will often be written into one lines, I broke them out so they're hopefully easier to read. They would also likely be much faster if we were to vectorize the code instead of using loops, but again, this is easy to read. \n",
    "\n",
    "#### Stop! In the Name of Words. Before you Break my Model.\n",
    "\n",
    "First, we will try to make a stop word tokenizer. If something is a stop word, we shall leave it out. As noted above, we can build this into the vectorizer, so why do it? This will allow for customizing the stopwords used - some applications may have a different usage of words, so changing stopwords makes sense. It is also a super fun exercise! \n",
    "\n",
    "This is also the most simple example we can try :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class swTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(tok)\n",
    "        return filtered_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization are similar - they both aim to break words down to their \"root\". For example, the word \"shoes\" probably has the same meaning as the word \"shoe\" for our purposes. Each approaches this in a slightly different way, and to understand that we need to take a look at the conecpt of similarity, which we'll look at more next time. \n",
    "\n",
    "#### Similarity\n",
    "\n",
    "When processing text, we can think of things being similar in two different ways - similar text or similar meaning - or lexical and semantic similarity. Things that are lexically similar use similar words, things that are semantically similar have similar meanings, even if the words are different. The stemming techniques here aren't explicit comparisons of those similarity types, but they follow the same concepts. Stemming breaks words down to their lexical root, lemmatization tries to find the semantic root.\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Stemming is the most simple, it just removes common prefixes and suffixes to extract the root of the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stemTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import SnowballStemmer\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(self.stemmer.stem(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Lemmatization is slightly more sophisticated, it attempts to find the semantic root, called the lemma, of a word using a search of a dictionary (we provide one from NLTK). For example, the lemma of \"better\" is \"good\". This is a \"smarter\" approach that the more simple stemming function above, but it is also more complex and slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                filtered_tok.append(self.lemmatizer.lemmatize(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions with More Processing and Grid Search\n",
    "\n",
    "We can try to see which processing setup works best, the winner will depend on how the text is written. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/l4n1h3z533nbg7ks96r5chn80000gn/T/ipykernel_1262/1326634129.py:25: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  grid.fit(X_train, y_train.ravel())\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 120 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n                   ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 110, in _analyze\n    doc = tokenizer(doc)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/j5/l4n1h3z533nbg7ks96r5chn80000gn/T/ipykernel_1262/2508330332.py\", line 5, in __call__\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n    tokenizer = _get_punkt_tokenizer(language)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n    return PunktTokenizer(language)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py\", line 579, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/glennbarnes/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n                   ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 110, in _analyze\n    doc = tokenizer(doc)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/j5/l4n1h3z533nbg7ks96r5chn80000gn/T/ipykernel_1262/915686364.py\", line 7, in __call__\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n    tokenizer = _get_punkt_tokenizer(language)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n    return PunktTokenizer(language)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py\", line 579, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/glennbarnes/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n                   ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 110, in _analyze\n    doc = tokenizer(doc)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/j5/l4n1h3z533nbg7ks96r5chn80000gn/T/ipykernel_1262/1444532330.py\", line 7, in __call__\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n    tokenizer = _get_punkt_tokenizer(language)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n    return PunktTokenizer(language)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py\", line 579, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/glennbarnes/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m\n\u001b[1;32m     14\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvect__max_features\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m500\u001b[39m,\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m1500\u001b[39m],\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvect__tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:(swTokenizer(stop_words), stemTokenizer(stop_words), lemmaTokenizer(stop_words) ),\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvect__norm\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m             }\n\u001b[1;32m     19\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator  \u001b[38;5;241m=\u001b[39m pipe2, \n\u001b[1;32m     20\u001b[0m                                param_grid \u001b[38;5;241m=\u001b[39m params, \n\u001b[1;32m     21\u001b[0m                                scoring    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                                cv         \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     23\u001b[0m                                n_jobs     \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m grid\u001b[38;5;241m.\u001b[39mfit(X_train, y_train\u001b[38;5;241m.\u001b[39mravel())\n\u001b[1;32m     26\u001b[0m best \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     27\u001b[0m preds \u001b[38;5;241m=\u001b[39m best\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:995\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    993\u001b[0m     )\n\u001b[0;32m--> 995\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_score)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 120 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n                   ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 110, in _analyze\n    doc = tokenizer(doc)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/j5/l4n1h3z533nbg7ks96r5chn80000gn/T/ipykernel_1262/2508330332.py\", line 5, in __call__\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n    tokenizer = _get_punkt_tokenizer(language)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n    return PunktTokenizer(language)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py\", line 579, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/glennbarnes/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n                   ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 110, in _analyze\n    doc = tokenizer(doc)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/j5/l4n1h3z533nbg7ks96r5chn80000gn/T/ipykernel_1262/915686364.py\", line 7, in __call__\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n    tokenizer = _get_punkt_tokenizer(language)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n    return PunktTokenizer(language)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py\", line 579, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/glennbarnes/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 469, in fit\n    Xt = self._fit(X, y, routed_params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 406, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/pipeline.py\", line 1310, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 2091, in fit_transform\n    X = super().fit_transform(raw_documents)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1372, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 1259, in _count_vocab\n    for feature in analyze(doc):\n                   ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py\", line 110, in _analyze\n    doc = tokenizer(doc)\n          ^^^^^^^^^^^^^^\n  File \"/var/folders/j5/l4n1h3z533nbg7ks96r5chn80000gn/T/ipykernel_1262/1444532330.py\", line 7, in __call__\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 142, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 119, in sent_tokenize\n    tokenizer = _get_punkt_tokenizer(language)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py\", line 105, in _get_punkt_tokenizer\n    return PunktTokenizer(language)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1744, in __init__\n    self.load_lang(lang)\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py\", line 1749, in load_lang\n    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py\", line 579, in find\n    raise LookupError(resource_not_found)\nLookupError: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/glennbarnes/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n\n"
     ]
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer()\n",
    "\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipe2 = Pipeline([ \n",
    "                    #(\"vect\", vec_cv),\n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", model_svc)\n",
    "])\n",
    "\n",
    "params = {\"vect__max_features\":[100,500,1000,1500],\n",
    "            \"vect__tokenizer\":(swTokenizer(stop_words), stemTokenizer(stop_words), lemmaTokenizer(stop_words) ),\n",
    "            \"vect__norm\":[\"l1\",\"l2\"]\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(estimator  = pipe2, \n",
    "                               param_grid = params, \n",
    "                               scoring    = \"balanced_accuracy\",\n",
    "                               cv         = 5,\n",
    "                               n_jobs     =-1)\n",
    "\n",
    "grid.fit(X_train, y_train.ravel())\n",
    "best = grid.best_estimator_\n",
    "preds = best.predict(X_test)\n",
    "print(best)\n",
    "print(classification_report(y_test, preds))\n",
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "We are accurate! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Categorize the following newsgroups. The data are posts from different newgroup boards. Try to categorize the data in either the atheism or religion groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "categories = [\"alt.atheism\", \"talk.religion.misc\"]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "X_train3 = data_train.data\n",
    "y_train3 = data_train.target\n",
    "X_test3 = data_test.data\n",
    "y_test3 = data_test.target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
